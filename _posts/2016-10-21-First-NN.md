---
layout: post

title:  "A Neural Networks from scratch"

date:   2016-10-20

published: true

category: general
---




We write a short and relly basic neural network. There are many from scracth neural networks outthere, one I particularly like is by [Denny Brtiz](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/).
Our goal is to build a three layer neural network. Where the first layer is the input layer, then we have the hidden layer and finally we get the output layer. To make our life easier and so we can visualize what's going on we restrict our output to points in $$\mathbb{R}^2$$ and out outputs to the set $$\{0,1\}$$.

Before diving into the procedure, let's import the packages we will use.


{% highlight ruby %}
# Packages
import matplotlib.pyplot as plt
import numpy as np
import matplotlib

# Display plots inline and change default figure size
%matplotlib inline
matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
{% endhighlight %}

## The activation function

In our previous notebook we used the logistic function $$ \varphi(x) = \frac{1}{1+e^{-x}}$$ as an activation function. The main reason for this choice is the fact that $$\varphi'(x)=\varphi(x)(1-\varphi(x))$$, which allows us to compute the weights of the neurons efficiently. But the sigmoid function has many drawbacks, one of them being that once the value at which activated is a little large, the slope of the gradient will be quite small, this makes gradient descent convergence to be quite slow. Lucklily there are some other function we can use that have a similar behavior, among them we have the hyperbolic tangent $$\tanh(x)=\frac{e^{2x}-1}{e^{2x}+1}.$$ Note that $$\tanh'(x)=1-\tanh^2(x)$$.

## What arguments to use?

In creating/training our neural network there are some variables involved. Say, the number of neurons, how many times we will go throught the training points, (aka the number of steps we are taking in the descend), are we doing full batch, mini-batch, or online version, and the learning rate. But, we are missing the most important part, the training Data!
We will get our training data "ramdomly", we can do this via the models in sklearn and choose some data that is bad for binary threshold, say a circle inside a circle.


{% highlight ruby %}
# Generate a dataset and plot it
from sklearn.datasets import make_circles
np.random.seed(13)
X, y = make_circles(200, noise=0.15)
plt.scatter(X[:,0], X[:,1], s=50, c=y, cmap=plt.cm.Set3);
{% endhighlight %}

<img src="{{ '/assets/img/Our_first_NN_files/Our_first_NN_8_0.png' | prepend: site.baseurl }}" alt=""> 


## The output doesn't feel right

And it shouldn't. We want the output to be either 0 or 1, but the output neuron has an activation function given by $$\varphi(x)$$ with continuous output in $$(0,1)$$, or an activation function $$\tanh(x)$$ with continuous output in $$(-1,1)$$. How do we fix this? When classifying it is common to understand a continuous output as a probability (think logistic), so we only need to change our output to the interval $$(0,1)$$, there are several ways of doing this, for example using [softmax](https://en.wikipedia.org/wiki/Softmax_function), using softmax introduces another problem for us, how to measure error! As we don't want to worry about this, we just do a naive approach, we change our desired targets from $$\{0,1\}$$ to $$\{-1,1\}$$.

## Some notation

Let's fix some notation. 
  
- $$W_h\in \mathbb{R}^{\text{hidden_size }\times 2}$$ = weights of each of the neurons in the hidden layer.
- $$b_h\in \mathbb{R}^{\text{hidden_size}}$$ = bias term in hidden units.
- $$W_o\in \mathbb{R}^{\text{hidden_size}}$$ = weights on the outcome unit. 
- $$b_o \in \mathbb{R}$$ = bias term in output unit.

Then we have the following equations, that we use for **forward propagation**,

- $$a_h= W_h\cdot x + b_h$$.
- $$y_h=f(a_h)$$.
- $$a_o=W_o\cdot y_h+b_0$$.
- $$y_o=f(a_o)$$.

Where $$f$$ is the activation function, $$x\in \mathbb{R}^2$$ is the input data, and $$y_o\in \mathbb{R}$$ is the output of the model.

## Backpropagation.

| Hyperbolic Tangent function   | 
|:-------:|
|$$\delta_o=(\hat{y}-y_o)(1-y_o^2)$$|
|$$\delta_h=(1-y^2_h)$$|
|$$\frac{\partial E}{\partial W_0}=\delta_o y_h$$ |
|$$\frac{\partial E}{\partial b_0}=\delta_o$$ |
|$$\frac{\partial E}{\partial W_h}=\delta_o W_o\delta_hx$$ | 
|$$\frac{\partial E}{\partial b_h}=\delta_oW_o\delta_h$$ | 

**IMPORTANT:** The vectors and matrix multiplication described on the table are component wise, in particular they are not given by dot product. 

## Implementation

We are ready for the implementation:


{% highlight ruby %}
# The activation functions:

from numpy import tanh

def sigmoid(x):
    return 1/(1+np.exp(-x))

#Our conventions
dim_input=2
dim_output=1


def ourFirstNN(X,y,num_passes=30000, hidden_size=5, activation='tanh',learning_rate=0.01):
    
    #Let's get some info
    data_size=len(X)
    
    
    
    # Initialize the parameters to random values. 
    
    np.random.seed(13)
    
    
    
    W_h = np.random.randn(dim_input, hidden_size) / np.sqrt(dim_input)
    b_h = np.zeros((1, hidden_size))
    W_o = np.random.randn(hidden_size, 1) / np.sqrt(hidden_size)
    b_o = np.zeros((1,1))
    
    #Let's keep track of sizes: 
    #x is (2,)
    #W_h is (2,n)
    #b_h is (1,n)
    #W_o is (n,1)
    #b_0 is (1,1)
    
    if activation=='tanh':
        
        #Converting our outcome y to {-1,1}
        y=np.where(y==1,1,-1)
        for i in range(num_passes):
            
            #pick a random training data element
            randInt=np.random.randint(0,len(X))
            x=X[randInt]
            y_hat=y[randInt]
            
            
            #Forward propagation
            a_h=x.dot(W_h)+b_h
            y_h=tanh(a_h)
            a_o=y_h.dot(W_o)+b_o
            y_o=tanh(a_o)
            
            #Let's keep track of sizes: 
            #a_h is (1,2)\cdot(2,n)+(1,n)=(200,n) (By broadcasting)
            #y_h is (1,n)
            #a_o is (1,n)\cdot(n,1)+(1,1)=(200,1) (By broadcasting)
            #y_o is (1,1)
            
            
            #BackPropagation
            delta_o=(y_o-y_hat)*(1-y_o*y_o)
            delta_h=(1-y_h*y_h)
            dEdW_o=delta_o*y_h
            dEdb_o=delta_o
            dEdW_h=(((delta_o.T*W_o)*delta_h.T)*(x.T)).T 
            dEdb_h=(((delta_o.T*W_o)*delta_h.T)).T 

            
            #Let's keep track of sizes: 
            #delta_o is (1,1)
            #delta_h is (1,n)
            #dEdW_o is (1,1)(1,n)=(1,n)
            #dEdb_o is (1,1)
            #dEdW_h is (2,n)
            #dEdb_h is (1,n)

            #Updating the values
            # Gradient descent parameter update
            W_o += -learning_rate * (dEdW_o.T)
            b_o += -learning_rate * dEdb_o
            W_h += -learning_rate * dEdW_h
            b_h += -learning_rate * dEdb_h
            
        
        #giving the model
        return { 'W_o': W_o, 'b_o': b_o, 'W_h': W_h, 'b_h': b_h}
        
    elif activation=='sigmoid':
        
        #You should do this as an exercise, just follow the same steps that we did for hyperbolic tangent.
        pass
    
        
    
{% endhighlight %}

In order to test our model, we need to evalue it, we can use the forward propagation piece of code for this.


{% highlight ruby %}
def predict(x,W_o,b_o,W_h,b_h):
    
    a_h=x.dot(W_h)+b_h
    y_h=tanh(a_h)
    a_o=y_h.dot(W_o)+b_o
    y_o=tanh(a_o)
    
    return y_o
{% endhighlight %}

We are amost ready to see how we did. We borrow the plot function from [Denny Britz](https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb).


{% highlight ruby %}
def plot_decision_boundary(pred_func):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    frame1 = plt.gca()
    frame1.axes.get_xaxis().set_visible(False)
    frame1.axes.get_yaxis().set_visible(False)
    plt.contourf(xx, yy, Z, cmap=plt.cm.cool)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.cool)

{% endhighlight %}

Next we look at some interesting models, and see what happens when we change the different parameters of our training.


{% highlight ruby %}
fig=plt.figure()
for i in range(4):
    for j in range(3):
        ax0 = plt.subplot2grid((3,5), (j,i))
        model=ourFirstNN(X,y,num_passes=5000*(i+1), hidden_size=3*(j+1), activation='tanh',learning_rate=0.01)
        W_o=model['W_o']
        b_o=model['b_o']
        W_h=model['W_h']
        b_h=model['b_h']
        def pred_func(x):
            return predict(x,W_o,b_o,W_h,b_h)

        plot_decision_boundary(pred_func)
        plt.title(str((i+1)*5000)+" data points \n "+str(3*(j+1))+" neurons.")
plt.tight_layout()
{% endhighlight %}

<img src="{{ '/assets/img/Our_first_NN_files/Our_first_NN_24_0.png' | prepend: site.baseurl }}" alt=""> 



Where the beautiful graphs, are giving just how close a point is from being -1 (yellow) or 1 (blue). If we want to find the regions, we will get the following.


{% highlight ruby %}
fig=plt.figure()
for i in range(4):
    for j in range(3):
        ax0 = plt.subplot2grid((3,5), (j,i))
        model=ourFirstNN(X,y,num_passes=5000*(i+1), hidden_size=3*(j+1), activation='tanh',learning_rate=0.01)
        W_o=model['W_o']
        b_o=model['b_o']
        W_h=model['W_h']
        b_h=model['b_h']
        def pred_func(x):
            return np.where(predict(x,W_o,b_o,W_h,b_h)<0,-1,1)
        plot_decision_boundary(pred_func)
        plt.title(str((i+1)*5000)+" data points \n "+str(3*(j+1))+" neurons.")
plt.tight_layout()
{% endhighlight %}

<img src="{{ '/assets/img/Our_first_NN_files/Our_first_NN_26_0.png' | prepend: site.baseurl }}" alt=""> 


Isn't this amazing? 
