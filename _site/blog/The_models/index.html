<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>The models</title>

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="Felipe's Place" href="http://scy1505.github.io/feed.xml" />
    
    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="http://scy1505.github.io/blog/The_models/">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>

     <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-58263416-1', 'auto');
ga('send', 'pageview');

</script>
 
</head>


<body>

	 <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/"> <font color="#5B0000">Felipe's Place</font></a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                 <li class="element first  ">
                     <a href="/index.html">Home</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/about">About</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/contact">Contact</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/research">Research</a>
                 </li> 
                 
                 <li class="element   last">
                     <a href="/articles">Posts</a>
                 </li> 
                 
                 <!--<li>
                     <a href="/articles">Posts</a>
                 </li>-->
                 <li> <a href="https://github.com/scy1505" target="_blank">GitHub</a></li>
                 <!-- <li><a href="https://github.com/brianmaierjr/long-haul/archive/master.zip">Download Theme</a></li> -->
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   	<div class="content">

   		
		 <div class="container">
        	 <div class="post">
  
  <h1 class="postTitle">The models</h1>
  <p class="meta">September 20, 2017 | <span class="time">14</span> Minute Read</p>
  
  <p>This is the last part on the series of the Amazon review dataset. In our <a href="/blog/Preparing_the_data/">previous post</a> we ready the data for training. In this post we build, train, and evaluate our models.</p>

<h1 id="part-4---model-building-and-the-results">Part 4 - Model Building and the results</h1>

<p>After loading the  data we created in our previous post. We note that there are about 500 features and 8 possible outcomes. There are a couple features that are not numerical, like daysSinceFirstReview, so we drop them. It is easy then to separate the features from the predictions.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">y_cats</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">y_cats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="no">Xy_df</span><span class="p">[</span><span class="s1">'salesRank_Category_of_'</span><span class="o">+</span><span class="n">str</span><span class="p">(</span><span class="n">i</span><span class="p">)].</span><span class="nf">as_matrix</span><span class="p">()</span>
    
<span class="no">X_df</span><span class="o">=</span><span class="no">Xy_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="s1">'salesRank'</span><span class="p">,</span><span class="s1">'asin'</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">'salesRank_Category_of_'</span><span class="o">+</span><span class="n">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">11</span><span class="p">)],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="no">X</span><span class="o">=</span><span class="no">X_df</span><span class="p">.</span><span class="nf">as_matrix</span><span class="p">()</span></code></pre></figure>

<h1 id="some-models">Some models</h1>

<p>We are going to use two different assembler classifiers to create the models.</p>

<h3 id="random-forest">Random Forest</h3>

<p>The first model we try is Random Forest, we tune n_estimators and max_features hyperparameters using a CVGridSearch approach. This will be running for a while. We also save our best models on file for future use. Keep in mind that these models are quite large and it may take a couple GB of space.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'n_estimators'</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">],</span> <span class="s1">'max_features'</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">]}</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">scores_RandomForest</span><span class="o">=</span><span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Training for %d bins'</span><span class="o">%</span><span class="n">i</span><span class="p">,</span><span class="k">end</span><span class="o">=</span><span class="s1">'\r'</span><span class="p">)</span>
    
    <span class="no">X_train</span><span class="p">,</span> <span class="no">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y_cats</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    
    <span class="n">bstRF</span><span class="o">=</span> <span class="no">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
    
    <span class="n">bstRF</span> <span class="o">=</span> <span class="no">GridSearchCV</span><span class="p">(</span><span class="n">randomForestModel</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">bstRF</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">"For the problem of classifying into %d bins the best parameters are"</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">bstRF</span><span class="p">.</span><span class="nf">best_params_</span><span class="p">)</span>
    
    <span class="n">scores_RandomForest</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">bstRF</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="no">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    
    <span class="c1">#We save the model for future use.</span>
    <span class="n">pickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">bstRF</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'./clf/videoGames_RF_'</span><span class="o">+</span><span class="n">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="s1">'wb'</span><span class="p">))</span></code></pre></figure>

<p>We skip running this block since it would take too much time on my laptop and I have better uses for it. The result for the Random Forest without tunning are as follows</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Number of bins</th>
      <th style="text-align: center">Accuracy for all product</th>
      <th style="text-align: center">Accuracy for products with more than 10 reviews</th>
      <th style="text-align: center">Random</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.8289738</td>
      <td style="text-align: center">0.8929765</td>
      <td style="text-align: center">0.5</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.7014587</td>
      <td style="text-align: center">0.8394648</td>
      <td style="text-align: center">0.333</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.6073943</td>
      <td style="text-align: center">0.7558528</td>
      <td style="text-align: center">0.25</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.5181086</td>
      <td style="text-align: center">0.6789297</td>
      <td style="text-align: center">0.2</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">0.4635311</td>
      <td style="text-align: center">0.6688963</td>
      <td style="text-align: center">0.166</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">0.4305835</td>
      <td style="text-align: center">0.6086956</td>
      <td style="text-align: center">0.142</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">0.3827967</td>
      <td style="text-align: center">0.5785953</td>
      <td style="text-align: center">0.125</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">0.3516096</td>
      <td style="text-align: center">0.5719063</td>
      <td style="text-align: center">0.111</td>
    </tr>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">0.3277162</td>
      <td style="text-align: center">0.5719063</td>
      <td style="text-align: center">0.1</td>
    </tr>
  </tbody>
</table>

<p>Which can be better seen in a graph.</p>

<center>
<img src="/assets/img/The_models_files/The_models_20_0.png" alt="" /> 
</center>

<h3 id="xgboost">Xgboost</h3>

<p>In the next step we try XGBoost since it usually gives better results. We show how to optimize Xgboost in the next section, we don’t do this for all the classifiers since this is high time/resources consuming.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">scores_XGB</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Training for %d bins'</span><span class="o">%</span><span class="n">i</span><span class="p">,</span><span class="k">end</span><span class="o">=</span><span class="s1">'\r'</span><span class="p">)</span>
    <span class="no">X_train</span><span class="p">,</span> <span class="no">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y_cats</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">xgbModel</span> <span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">()</span>
    <span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()[</span><span class="s1">'num_class'</span><span class="p">]</span><span class="o">=</span><span class="n">i</span>
    <span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()[</span><span class="s1">'objective'</span><span class="p">]</span><span class="o">=</span><span class="s2">"multi:softmax"</span>
    <span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()[</span><span class="s1">'learning_rate'</span><span class="p">]</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span>
    <span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()[</span><span class="s1">'n_estimators'</span><span class="p">]</span><span class="o">=</span><span class="mi">140</span>
    <span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()[</span><span class="s1">'max_depth'</span><span class="p">]</span><span class="o">=</span><span class="mi">5</span>
    <span class="n">xgbModel</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,)</span>
    <span class="n">scores_XGB</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">xgbModel</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="no">X_test</span><span class="p">)))</span>
    
    <span class="n">pickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">xgbModel</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'./clf/videoGames_XGB_'</span><span class="o">+</span><span class="n">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="s1">'wb'</span><span class="p">))</span></code></pre></figure>

<p>Which gives the results described in the following graph.</p>

<center>
<img src="/assets/img/The_models_files/The_models_27_0.png" alt="" /> 
</center>

<p>Maybe more interesting if we put all the model together, note that we definitely get a much better prediction for products that have more reviews. But even in the case of a low number of reviews we see that the reviews are indeed an indicative of the product performance.</p>

<center>
<img src="/assets/img/The_models_files/The_models_29_0.png" alt="" /> 
</center>

<p>Another thing we notice, is that the XGBoost is not much better than the random Forest, maybe because we haven’t tune the hyperparameters.</p>

<h1 id="hyperparameter-tunning">Hyperparameter tunning.</h1>

<p>In this section we show how to optimize the hyperparameters for the xgboost model. We choose the smaller dataset with 10 bins, and separate into training and testing.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">y</span><span class="o">=</span><span class="n">y_cats_10</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span>
<span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span> <span class="k">if</span> <span class="n">val</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">'10'</span> <span class="k">else</span> <span class="n">int</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">val</span> <span class="k">in</span> <span class="n">y</span><span class="p">]</span>
<span class="no">X_train</span><span class="p">,</span> <span class="no">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="no">X_10</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span></code></pre></figure>

<p>The next step is to realize the order of magnitude of estimators that we need, to do this we first create a XGBoost model</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">xgbModel</span> <span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">(</span>
 <span class="n">learning_rate</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
 <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
 <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
 <span class="n">subsample</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
 <span class="n">colsample_bytree</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
 <span class="n">objective</span><span class="o">=</span> <span class="s2">"multi:softmax"</span><span class="p">,</span>
 <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
 <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()[</span><span class="s1">'num_class'</span><span class="p">]</span><span class="o">=</span><span class="mi">10</span></code></pre></figure>

<p>and use cross validation to find when then number of estimator begin to be not make change in our accuracy.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">xgb_param</span> <span class="o">=</span> <span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_xgb_params</span><span class="p">()</span>
<span class="n">xgtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="no">DMatrix</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">xgb_param</span><span class="p">[</span><span class="s1">'num_class'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">cvresult</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nf">cv</span><span class="p">(</span>
    <span class="n">xgb_param</span><span class="p">,</span> 
    <span class="n">xgtrain</span><span class="p">,</span> 
    <span class="n">num_boost_round</span><span class="o">=</span><span class="n">xgbModel</span><span class="p">.</span><span class="nf">get_params</span><span class="p">()[</span><span class="s1">'n_estimators'</span><span class="p">],</span> 
    <span class="n">nfold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code></pre></figure>

<p>We obtain that we need about 102 estimators. So we go with this in order to find the other parameters. We first go with the max_depth and min_child_weight.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">param_test1</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s1">'max_depth'</span><span class="ss">:range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
 <span class="s1">'min_child_weight'</span><span class="ss">:range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">gsearch1</span> <span class="o">=</span> <span class="no">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">(</span> <span class="n">learning_rate</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">102</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
 <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
 <span class="n">objective</span><span class="o">=</span> <span class="s2">"multi:softmax"</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">),</span> 
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test1</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="no">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">gsearch1</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gsearch1</span><span class="p">.</span><span class="nf">grid_scores_</span><span class="p">,</span> <span class="n">gsearch1</span><span class="p">.</span><span class="nf">best_params_</span><span class="p">,</span> <span class="n">gsearch1</span><span class="p">.</span><span class="nf">best_score_</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>([mean: 0.54573, std: 0.02161, params: {'min_child_weight': 1, 'max_depth': 3},
  mean: 0.54203, std: 0.02513, params: {'min_child_weight': 3, 'max_depth': 3},
  mean: 0.55095, std: 0.01680, params: {'min_child_weight': 5, 'max_depth': 3},
  mean: 0.54201, std: 0.01478, params: {'min_child_weight': 1, 'max_depth': 5},
  mean: 0.54574, std: 0.01066, params: {'min_child_weight': 3, 'max_depth': 5},
  mean: 0.54648, std: 0.01484, params: {'min_child_weight': 5, 'max_depth': 5},
  mean: 0.55169, std: 0.01954, params: {'min_child_weight': 1, 'max_depth': 7},
  mean: 0.54761, std: 0.01079, params: {'min_child_weight': 3, 'max_depth': 7},
  mean: 0.54275, std: 0.00697, params: {'min_child_weight': 5, 'max_depth': 7},
  mean: 0.54909, std: 0.01307, params: {'min_child_weight': 1, 'max_depth': 9},
  mean: 0.54536, std: 0.00898, params: {'min_child_weight': 3, 'max_depth': 9},
  mean: 0.54389, std: 0.00756, params: {'min_child_weight': 5, 'max_depth': 9}],
 {'max_depth': 7, 'min_child_weight': 1},
 0.55169490036143287)
</code></pre>
</div>

<p>Next we cross validate for gamma</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">param_test2</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s1">'gamma'</span><span class="p">:[</span><span class="n">i</span><span class="o">/</span><span class="mi">10</span><span class="o">.</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)]</span>
<span class="p">}</span>
<span class="n">gsearch2</span> <span class="o">=</span> <span class="no">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">(</span> <span class="n">learning_rate</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">110</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
 <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
 <span class="n">objective</span><span class="o">=</span> <span class="s2">"multi:softmax"</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">27</span><span class="p">),</span> 
 <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test2</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="no">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">gsearch2</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gsearch2</span><span class="p">.</span><span class="nf">grid_scores_</span><span class="p">,</span> <span class="n">gsearch2</span><span class="p">.</span><span class="nf">best_params_</span><span class="p">,</span> <span class="n">gsearch2</span><span class="p">.</span><span class="nf">best_score_</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>([mean: 0.54836, std: 0.01822, params: {'gamma': 0.0},
  mean: 0.54910, std: 0.01418, params: {'gamma': 0.1},
  mean: 0.55655, std: 0.01242, params: {'gamma': 0.2},
  mean: 0.54872, std: 0.01457, params: {'gamma': 0.3},
  mean: 0.54426, std: 0.00920, params: {'gamma': 0.4}],
 {'gamma': 0.2},
 0.55654553708434285)
</code></pre>
</div>

<p>follow by the subsample size and the colsample_bytree</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">param_test3</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s1">'subsample'</span><span class="p">:[</span><span class="n">i</span><span class="o">/</span><span class="mi">10</span><span class="o">.</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)],</span>
 <span class="s1">'colsample_bytree'</span><span class="p">:[</span><span class="n">i</span><span class="o">/</span><span class="mi">10</span><span class="o">.</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="p">}</span>
<span class="n">gsearch3</span> <span class="o">=</span> <span class="no">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">(</span> <span class="n">learning_rate</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">110</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
 <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
 <span class="n">objective</span><span class="o">=</span> <span class="s2">"multi:softmax"</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">27</span><span class="p">),</span> 
 <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test3</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="no">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">gsearch3</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gsearch3</span><span class="p">.</span><span class="nf">grid_scores_</span><span class="p">,</span> <span class="n">gsearch3</span><span class="p">.</span><span class="nf">best_params_</span><span class="p">,</span> <span class="n">gsearch3</span><span class="p">.</span><span class="nf">best_score_</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>([mean: 0.53828, std: 0.00689, params: {'subsample': 0.6, 'colsample_bytree': 0.6},
  mean: 0.54908, std: 0.01704, params: {'subsample': 0.7, 'colsample_bytree': 0.6},
  mean: 0.54127, std: 0.01434, params: {'subsample': 0.8, 'colsample_bytree': 0.6},
  mean: 0.53792, std: 0.01848, params: {'subsample': 0.9, 'colsample_bytree': 0.6},
  mean: 0.54127, std: 0.01538, params: {'subsample': 0.6, 'colsample_bytree': 0.7},
  mean: 0.54874, std: 0.01445, params: {'subsample': 0.7, 'colsample_bytree': 0.7},
  mean: 0.54835, std: 0.00875, params: {'subsample': 0.8, 'colsample_bytree': 0.7},
  mean: 0.54202, std: 0.01154, params: {'subsample': 0.9, 'colsample_bytree': 0.7},
  mean: 0.54539, std: 0.01139, params: {'subsample': 0.6, 'colsample_bytree': 0.8},
  mean: 0.54650, std: 0.01588, params: {'subsample': 0.7, 'colsample_bytree': 0.8},
  mean: 0.55655, std: 0.01242, params: {'subsample': 0.8, 'colsample_bytree': 0.8},
  mean: 0.54276, std: 0.01903, params: {'subsample': 0.9, 'colsample_bytree': 0.8},
  mean: 0.54646, std: 0.01259, params: {'subsample': 0.6, 'colsample_bytree': 0.9},
  mean: 0.54685, std: 0.01228, params: {'subsample': 0.7, 'colsample_bytree': 0.9},
  mean: 0.54462, std: 0.01579, params: {'subsample': 0.8, 'colsample_bytree': 0.9},
  mean: 0.54203, std: 0.01190, params: {'subsample': 0.9, 'colsample_bytree': 0.9}],
 {'colsample_bytree': 0.8, 'subsample': 0.8},
 0.55654553708434285)
</code></pre>
</div>

<p>So, it seems that the ones we started with were already a good choice. Finally, we do the reg_alpha</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">param_test4</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s1">'reg_alpha'</span><span class="p">:[</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">gsearch4</span> <span class="o">=</span> <span class="no">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">(</span> <span class="n">learning_rate</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">110</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
 <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
 <span class="n">objective</span><span class="o">=</span> <span class="s2">"multi:softmax"</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">),</span> 
 <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test4</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="no">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">gsearch4</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gsearch4</span><span class="p">.</span><span class="nf">grid_scores_</span><span class="p">,</span> <span class="n">gsearch4</span><span class="p">.</span><span class="nf">best_params_</span><span class="p">,</span> <span class="n">gsearch4</span><span class="p">.</span><span class="nf">best_score_</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>([mean: 0.54461, std: 0.01104, params: {'reg_alpha': 1e-05},
  mean: 0.54272, std: 0.01684, params: {'reg_alpha': 0.01},
  mean: 0.54313, std: 0.01283, params: {'reg_alpha': 0.1},
  mean: 0.54870, std: 0.01768, params: {'reg_alpha': 1},
  mean: 0.44503, std: 0.01289, params: {'reg_alpha': 100}],
 {'reg_alpha': 1},
 0.54870361819532376)
</code></pre>
</div>

<p>As the last step we decrease the learning rate and increase the number of estimators to get our classifier.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">bstXGB</span><span class="o">=</span> <span class="no">XGBClassifier</span><span class="p">(</span>
    <span class="n">learning_rate</span> <span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mo">01</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">subsample</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span> <span class="s2">"multi:softmax"</span><span class="p">,</span> 
    <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
    <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>
    <span class="n">reg_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">bstXGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span></code></pre></figure>

<p>and use the testing data to see note that we get an accuracy of 0.57525083612040129 which compared with the score of 0.53846153846153844 we got before, it is a quite decent improvement. (Note that this is about the same we got with Random Forest).</p>

<h1 id="final-notes">Final Notes</h1>

<p>The reviews dataset contains important information about the salesRank, as such it can be used to give a prediction. Even, if the prediction is far from perfect we have shown that it can be used as a feature to improve our understanding of the products.</p>

<p>We also note that there are many different ways to improve the results, there are ranking ML techniques that may give better results (at the cost of longer training time), or we could mine the original data to get more features. From the pictures, from recommenders, from the similar products, etc.</p>

<p>In order to implement this in production, there are several ways to improve the code, which would turn with a higher performance for large datasets. Fortunately for us, the only feature that required heavy processing is the sentiment analysis, which can be performed offline. All the other techniques we used have a fast performance for larger datasets.</p>



  <!-- POST NAVIGATION -->
  <div class="postNav clearfix">
     
      <a class="prev" href="/blog/Preparing_the_data/"><span>&laquo;&nbsp;Preparing the data</span>
      
    </a>
      
      
      <a class="next" href="/blog/Intro_to_attention_mechanisms/"><span>Intro to Attention Mechanisms&nbsp;&raquo;</span>
       
      </a>
     
  </div>
</div>

      	</div>
      	
      	


	</div><!-- end .content -->


   <div class="footer">
   <div class="container">
      <p class="copy">&copy; 2017 <a href="https://scy1505.github.io">Felipe Pérez.</a> Powered by <a href="http://jekyllrb.com">Jekyll</a></p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/juan1505">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://twitter.com/jperezvallejo">
                  <svg id="twitter" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/scy1505">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="mailto:felipe.perez.ds@gmail.com">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->


  
   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



</body>

</html>
