<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>Neural Networks</title>

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="Puppy's Place" href="http://scy1505.github.io/feed.xml" />
    
    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="http://scy1505.github.io/blog/neural-nets/">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>

     <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-58263416-1', 'auto');
ga('send', 'pageview');

</script>
 
</head>


<body>

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/">Puppy's Place</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                 <li class="element first  ">
                     <a href="/index.html">Home</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/about">About</a>
                 </li> 
                 
                 <li class="element   last">
                     <a href="/contact">Contact</a>
                 </li> 
                 
                 <li> <a href="https://github.com/brianmaierjr/long-haul" target="_blank">GitHub</a></li>
                 <li><a href="https://github.com/brianmaierjr/long-haul/archive/master.zip">Download Theme</a></li>
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post">
  
  <h1 class="postTitle">Neural Networks</h1>
  <p class="meta">October 19, 2016 | <span class="time">11</span> Minute Read</p>
  
  <p>This notebook and the ones following are design to cover the basics of neural networks in python.</p>

<h1 id="section-1-introduction">Section 1: Introduction</h1>

<p>The goal of neural nets is to train neurons, such that the network learn to perfoform complext operations. In order to made this clear, we need to understand what a neuron is, what are the different models for a neuron, and how to train them.</p>

<h2 id="models-for-neurons">1.1 Models for Neurons.</h2>

<p>In essence, neurons are objects that for an input <script type="math/tex">(x_1,\ldots,x_n)</script> has an output <script type="math/tex">y</script>. A training set is a collection of inputs and corresponding outpuuts, a neuron is trained by the training set by finding the appropiate parameters that defined the neuron. There are several model for neurons. We study some of the basic ones next.</p>

<center><img src="/assets/img/neural_nets_1_files/image.png" alt="" /> </center>

<h3 id="linear-neurons">1.1.1 Linear Neurons</h3>

<p>For an input <script type="math/tex">(x_1,\ldots,x_n)</script> the linear neurons compute the output <script type="math/tex">y</script> by letting</p>

<script type="math/tex; mode=display">y=b+ w_1 x_1+w_2x_2+\ldots w_nx_n.</script>

<p>The training as we will see later consists on finding the right weights <script type="math/tex">w_1,\ldots,w_n</script> and the bias term <script type="math/tex">b</script>. It is common to make the problem homogeneous (this will make the training easier). That is to change the problem to a problem were there’s no bias term. We achieve this by considering the input to be <script type="math/tex">(x_1,\ldots,x_n,1)</script> and then the output is just a linear combination with weights <script type="math/tex">w_1,\ldots,w_n,1</script>.</p>

<h3 id="binary-thresholds-neurons">1.1.2 Binary Thresholds Neurons</h3>

<p>The previous class of neurons are good are good for regression problems, but for classification problems we need something with a discrete output. So consider instead a function depending of an extra parameter <script type="math/tex">\theta</script> such that</p>

<script type="math/tex; mode=display">% <![CDATA[
y=\begin{cases}
1& \text{if }w_1 x_1+w_2x_2+\ldots w_nx_n\geq \theta;\\
0 & \text{otherwise}.
\end{cases} %]]></script>

<p>Note that we can also homogenize this question so we may assume <script type="math/tex">\theta=0</script>.</p>

<h3 id="rectified-linear-neurons">1.1. 3 Rectified Linear Neurons</h3>

<p>-In this case the output is obtained as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
y&=\begin{cases}
w_1 x_1+w_2x_2+\ldots w_nx_n & \text{if  } w_1 x_1+w_2x_2+\ldots w_nx_n\geq 0;\\
0 & \text{otherwise}.
\end{cases} \\
& \\
&=\max\{0,w_1 x_1+w_2x_2+\ldots w_nx_n\}. 
\end{align*} %]]></script>

<p>These neurons have advantages, like improving speed in the training of deep neural networks.</p>

<h3 id="sigmoid-neurons">1.1.4 Sigmoid Neurons</h3>

<p>The sigmoid function ives a smooth output that is always between 0 and 1.</p>

<script type="math/tex; mode=display">y = \frac{1}{1+e^{-(w_1 x_1+w_2x_2+\ldots w_nx_n)}}.</script>

<p>Note what happens when $w_1 x_1+w_2x_2+\ldots w_nx_n\to \pm \infty$.</p>

<h3 id="stochastic-binary">1.1.5 Stochastic Binary</h3>

<p>This function treats the output of the Sigmoid as the probability of an outcome</p>

<script type="math/tex; mode=display">P(Y=1) = \frac{1}{1+e^{-(w_1 x_1+w_2x_2+\ldots w_nx_n)}}.</script>

<h2 id="perceptrons-the-network-of-one-neuron">1.2 Perceptrons: The network of one neuron.</h2>

<p>Perceptrons use Binary Threshold Neurons. Assume that we have some given some training data <script type="math/tex">(\boldsymbol{x_i},y_i)</script>, where <script type="math/tex">\boldsymbol{x_i}</script> represents an <script type="math/tex">n</script>-tuple and <script type="math/tex">$y_i\in \{0,1\}</script>. Note that a Binary Threshold Neurons corresponds to a hyperplane, that tries to separate the points with output 1, from the points with output 0. For example, suposse we want to classify the data <script type="math/tex">(-1,2)\mapsto 1</script> <script type="math/tex">(2,0)\mapsto 1</script> <script type="math/tex">(-1,-3)\mapsto 0</script> <script type="math/tex">(1,-2) \mapsto 0</script> The binary threshold neuron with weights <script type="math/tex">w_1=-1</script> and <script type="math/tex">w_2=1</script>, will incorrectly classify the points (fig. 1), meanwhile the binary threshold neuron with weights <script type="math/tex">w_1=1</script>, <script type="math/tex">w_2=1</script> (fig. 2) does classify them correctly.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">import</span> <span class="n">numpy</span> <span class="n">as</span> <span class="n">np</span>
<span class="n">import</span> <span class="n">matplotlib</span><span class="p">.</span><span class="nf">pyplot</span> <span class="n">as</span> <span class="n">plt</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="s1">'left'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="s1">'ro'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Output 1'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="s1">'bo'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Output 0'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="no">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">fill</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">fill</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s1">'fig. 1'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>


<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="s1">'left'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="s1">'ro'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Output 1'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="s1">'bo'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Output 0'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="no">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">fill</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">fill</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s1">'fig. 2'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">draw</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure>

<center><img src="/assets/img/neural_nets_1_files/neural_nets _1_18_0.png" alt="" /></center>

<p>So, the natural question is how do we obtain the right weights.</p>

<h3 id="training-the-perceptron">1.2.1 Training the Perceptron</h3>

<p>The algorithm for training the perceptron is a simple one. Choose a random vector of weights <script type="math/tex">\boldsymbol{w}=(w_1,\ldots,w_n)</script>, then go through the training data points. For every point <script type="math/tex">\boldsymbol{x_i}</script> do one of the following:</p>

<ul>
  <li>If <script type="math/tex">\boldsymbol{x_i}</script> is correctly classified, do nothing.</li>
  <li>If <script type="math/tex">\boldsymbol{x_i}</script> is incorrectly classified as <script type="math/tex">0</script>, then make <script type="math/tex">\boldsymbol{w}=\boldsymbol{w}+\boldsymbol{x_i}</script>.</li>
  <li>If <script type="math/tex">\boldsymbol{x_i}</script> is incorrectly classified as <script type="math/tex">1</script>, then make <script type="math/tex">\boldsymbol{w}=\boldsymbol{w}-\boldsymbol{x_i}</script>.</li>
</ul>

<p>Keep repeating this procedure, that is go trought all data points again. After posssibly going trhought the data points several times, we will obtain <script type="math/tex">\boldsymbol{w}</script> that separates the ones with output 0 from the ones with output 1.</p>

<p>At this moment you should be wondering why this works. First: It doesn’t always works, because there are sets that can not be classified, for example:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="s1">'left'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s1">'ro'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Output 1'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="s1">'bo'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Output 0'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="no">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s2">"Perceptrons can't always classify"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">draw</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span></code></pre></figure>

<center><img src="/assets/img/neural_nets_1_files/neural_nets_1_23_0.png" alt="" /></center>

<p>But, if there is a hyperplane separating the points with output 1 from the points with output 0, then this process finishes after finitely many steps. Unfortunately, the convergence tends to be slow.</p>

<h2 id="linear-neuron">1.3 Linear Neuron</h2>

<p>Given some training data, it is higly improbable that it will fit in linear neuron, so in this case we want is to minimize the error, we make this more precise next. Given some training data <script type="math/tex">(\boldsymbol{x_i},y_i)</script> where <script type="math/tex">y_i\in \mathbb{R}</script>, what is the vector <script type="math/tex">\boldsymbol{w}=(w_1,\ldots,w_n)</script> for which</p>

<script type="math/tex; mode=display">\frac{1}{2}\sum_i(y_i-\boldsymbol{w} \cdot \boldsymbol{x_i})^2=\text{Error}</script>

<p>is as small as possible. If we follow the analytic path we obtain the usual linear regresssion, instead we want to use something different, that will generalize to larger neuran networks.</p>

<h2 id="linear-neuron-training-full-batch">1.3.1 Linear Neuron Training (Full Batch)</h2>

<p>One algorithmic way to get to the min of a function is via gradient descent. We start with some initial weight vector <script type="math/tex">\boldsymbol{w_0}</script>, we choose a learning rate <script type="math/tex">\epsilon</script> and we set</p>

<script type="math/tex; mode=display">\boldsymbol{w_{t+1}}=\boldsymbol{w_{t}}+\Delta \boldsymbol{w_t}=\boldsymbol{w_{t}}-\epsilon \sum_i \boldsymbol{x_i}(y_i- \boldsymbol{w_t}\cdot \boldsymbol{x_i})</script>

<p>Choosing the learning $\epsilon$ small enough the sequence $\boldsymbol{w_t}$ converges to the analytic solution. This procedure has several shortcomings, one of them being that it relies in knowing all the data. Another approach is to use every new data to update the weight vector. We do this next.</p>

<h2 id="linear-neuron-training-online">1.3.2 Linear Neuron Training (Online)</h2>

<p>We ramdonly pick a training data element <script type="math/tex">(\boldsymbol{x_i},y_i)</script> and put it back, we use this element to update the weight vector as</p>

<script type="math/tex; mode=display">\boldsymbol{w_{t+1}}=\boldsymbol{w_{t}}+\Delta \boldsymbol{w_t}=\boldsymbol{w_{t}}-\epsilon \boldsymbol{x_i}(y_i- \boldsymbol{w_t}\cdot \boldsymbol{x_i})</script>

<p>This procedure has an extremely slow convergence rate (it zigzags towards the minimum).</p>

<p>One way to fix the shortcomming of both techniques is to use mini-batches, take random samples of small size and update using the formula for Full Batch.</p>

<h2 id="logistic-neurons">1.4 Logistic Neurons</h2>

<p>This is just another name for Sigmoid Neurons. Recall that in this case the output is computed as</p>

<p><script type="math/tex">y = \frac{1}{1+e^{-(w_1 x_1+w_2x_2+\ldots w_nx_n)}}.</script>
and we want to find the right weights $w_1,\ldots,w_n$. As we the linear case we want to use gradient descent to minimize</p>

<script type="math/tex; mode=display">\text{Error} = \frac{1}{2} \sum_i\left(y_i-\frac{1}{1+e^{-\boldsymbol{w}\cdot \boldsymbol{x_i}}}\right)^2.</script>

<p>And so the updating process  would go as</p>

<script type="math/tex; mode=display">\boldsymbol{w_{t+1}}=\boldsymbol{w_{t}}+\Delta \boldsymbol{w_t}=\boldsymbol{w_{t}}- \epsilon\sum_i \boldsymbol{x_i} \left( \frac{1}{1+e^{-\boldsymbol{w}\cdot \boldsymbol{x_i}}} \right)\left( 1-\frac{1}{1+e^{-\boldsymbol{w}\cdot \boldsymbol{x_i}}} \right) \left(y_i- \frac{1}{1+e^{-\boldsymbol{w}\cdot \boldsymbol{x_i}}} \right)</script>

<p>As above we could use a online training or a mini-batch, we leave the details to the reader.</p>

<h1 id="neural-networks">2. Neural Networks</h1>

<p>We move now to more complex systems. In general a neural network consists of a collection of neurons interconnected. The first kind of network we will study is that for whih each neuron has an activation function <script type="math/tex">y=f(x_1,\ldots,x_n)</script> that is smooth. For example, if <script type="math/tex">f(x)</script> is the sigmoid. Our goal is to ind the “best” weights for each of the neurons.</p>

<h2 id="backpropagation">2.1 Backpropagation</h2>

<p>We assume that the neural network is feedforward, this basically means that there are not cycles.</p>

<center><img src="/assets/img/neural_nets_1_files/image.png" alt="" /> </center>

<p>Back propagation is a technique to compute the local gradient efficiently, that is by how much the weights in each unit must change. The general explanation requieres some (annoying) notation, in order to make the ideas clear we consider a smaller case and let the reader imaging the general case. Consider the network given by the next figure.</p>

<center> <img src="/assets/img/neural_nets_1_files/image.png" alt="" /> </center>

<p>In this case we have on output unit, one hidden layer with two units, and one input. Our goal is to minimize the error <script type="math/tex">E=E(w_{1},w_{2},w_{31},w_{32})</script> where <script type="math/tex">w_{ij}</script> is the <script type="math/tex">j</script>-th weight of the <script type="math/tex">i</script>-th unit, we minimize <script type="math/tex">E</script> using gradient descend so we need to find <script type="math/tex">\frac{ \partial E}{\partial w_{ij}}</script>. Let</p>

<script type="math/tex; mode=display">\varphi(z)=\frac{1}{1+e^{-z}},</script>

<p>and <script type="math/tex">f_i</script> the sigmoid activation function associated to the <script type="math/tex">i</script>-th unit, that is <script type="math/tex">f_1(x)=\varphi(w_1x)</script>, <script type="math/tex">f_2(x)=\varphi(w_2x)</script>, and <script type="math/tex">f_3(a,b)=\varphi(w_{31}a+w_{32}b)</script>.</p>

<p>Note that given a data point <script type="math/tex">(x_1,x_2,y)</script> the error function is given by</p>

<script type="math/tex; mode=display">E=\frac{1}{2}\big (\varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big)-y \big )^2</script>

<p>Now, a trivial computation shows <script type="math/tex">\frac{\partial \varphi(z)}{ \partial z }=\varphi(z)(1-\varphi(z))</script>, and we can use the chain rule to get</p>

<script type="math/tex; mode=display">\begin{multline} 
\frac{\partial E}{\partial w_{3i}}= \big (\varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big)-y \big ) \cdot \big (1-\varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big)\big ) \\
 \cdot \varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big) \cdot f_i(x) 
\end{multline}</script>

<p>Meanwhile,</p>

<script type="math/tex; mode=display">\begin{multline} 
\frac{\partial E}{\partial w_{i}}= \big (\varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big)-y \big ) \cdot \big (1-\varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big)\big )  \\ 
  \cdot \varphi \big( w_{31}f_1(x)+w_{32}f_2(x) \big)\cdot w_{3i}\cdot f_i(x)\cdot (1-f_i(x))\cdot x  
\end{multline}</script>

<p>That is <script type="math/tex">\frac{\partial E}{\partial w_{i}} =w_{3i}\cdot x\cdot(1-f_i(x)) \cdot \frac{\partial E}{\partial w_{3i}}</script>
and we can compute the earlier errors from the later ones. We can use this, plus the ideas above, to reach a minimum, unfortunately we can only guarantee that this is a local minimum.</p>



  <!-- POST NAVIGATION -->
  <div class="postNav clearfix">
     
      <a class="prev" href="/blog/example/"><span>&laquo;&nbsp;The K-armed Bandit</span>
      
    </a>
      
      
      <a class="next" href="/blog/First-NN/"><span>A Neural Networks from scratch&nbsp;&raquo;</span>
       
      </a>
     
  </div>
</div>

      </div>
   </div><!-- end .content -->

   <div class="footer">
   <div class="container">
      <p class="copy">&copy; 2016 <a href="http://brianmaierjr.com">Brian Maier Jr.</a> Powered by <a href="http://jekyllrb.com">Jekyll</a></p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/juan1505">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://twitter.com/juan1505">
                  <svg id="twitter" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/scy1505">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="mailto:juan1505@gmail.com">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->


    <aside>
     <ul class="noList">
                 
                 <li class="element first  ">
                     <a href="/index.html">Home</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/about">About</a>
                 </li> 
                 
                 <li class="element   last">
                     <a href="/contact">Contact</a>
                 </li> 
                 
                 <li> <a href="https://github.com/brianmaierjr/long-haul" target="_blank">GitHub</a></li>
                 <li><a href="https://github.com/brianmaierjr/long-haul/archive/master.zip">Download Theme</a></li>
             </ul>
 </aside>
   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



</body>

</html>
